{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4a0lwxUeEKsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb687687-6c79-4d33-a013-fcbb44a98364"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wSkZiG4zBfKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb4fe87-5937-4547-b369-9fbc8dc53eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "# Load Wikitext-103 dataset\n",
        "dataset = load_dataset(path=\"wikitext\", name=\"wikitext-103-v1\", split=\"train\")\n",
        "# Define tokenizer\n",
        "tokenizer = lambda x: x.split()  # Simple tokenizer splitting by space\n",
        "\n",
        "# Tokenize the text and count frequency\n",
        "counter = Counter()\n",
        "train_data=[]\n",
        "for example in dataset:\n",
        "    tokens = tokenizer(example[\"text\"])\n",
        "    if example[\"text\"]!='':\n",
        "      train_data.append(example[\"text\"])\n",
        "    counter.update(tokens)\n",
        "\n",
        "# Select the 5000 most common words\n",
        "most_common_words = counter.most_common(5000)\n",
        "word_to_index = {word: i for i, (word, _) in enumerate(most_common_words)}\n",
        "\n",
        "# Define a function to convert text to one-hot encoding\n",
        "def text_to_one_hot(text):\n",
        "    token = text\n",
        "    index = word_to_index.get(token, -1)\n",
        "    return index\n",
        "\n",
        "# Convert example text into one-hot encoding as a torch tensor\n",
        "train_w_data = [text_to_one_hot(word) for sentence in train_data for word in sentence.split(' ')]\n",
        "train_data = [num for num in train_w_data if num!=-1]\n",
        "example_text = train_data[0]\n",
        "\n",
        "one_hot_encoding = text_to_one_hot(example_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Text_dataset(Dataset):\n",
        "\n",
        "  def __init__(self, text_list, text_len=30):\n",
        "    self.data= text_list\n",
        "    self.text_len = text_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)//self.text_len\n",
        "\n",
        "  # This will\n",
        "  def __getitem__(self,i):\n",
        "    one_hots = []\n",
        "    for j in range(i*self.text_len,i*self.text_len+self.text_len):\n",
        "      one_hots.append(self.data[j])\n",
        "    return torch.tensor(one_hots[:-1]), one_hots[-1]"
      ],
      "metadata": {
        "id": "gMGidKTys8Yo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_Gen(nn.Module):\n",
        "  def __init__(self,input_size=5000, hidden_size=1024, hidden_layer=1,embedding_size=512, batch_size=20):\n",
        "    super(LSTM_Gen, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden_layer = hidden_layer\n",
        "    self.embedding_size = embedding_size\n",
        "    self.embedding = torch.nn.Embedding(self.input_size, self.embedding_size)\n",
        "    self.lstm = torch.nn.LSTM(self.embedding_size, self.hidden_size, self.hidden_layer, batch_first=True)\n",
        "    self.fc1 = torch.nn.Linear(self.hidden_size,self.input_size)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden = (torch.zeros(self.hidden_layer, batch_size, self.hidden_size),torch.zeros(self.hidden_layer,batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embedding(x)\n",
        "    x, _ = self.lstm(x, self.hidden)\n",
        "    x = self.fc1(x)\n",
        "    out = self.relu(x)\n",
        "    return out[:,-1,:].squeeze(dim=1)\n"
      ],
      "metadata": {
        "id": "MtegyoPDt_p5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "batch_size = 20\n",
        "text_len = 30\n",
        "hidden_size = 1024\n",
        "hidden_layer = 1\n",
        "embedding_size = 512\n",
        "epoch = 5\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.0005\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Set device to CUDA\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available! Using GPU for training.\")\n",
        "else:\n",
        "    # Set device to CPU\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Using CPU for training.\")\n",
        "dset = Text_dataset(train_data, text_len)\n",
        "model = LSTM_Gen(5000,hidden_size,hidden_layer,embedding_size,batch_size)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(epoch):\n",
        "  train_loader = DataLoader(dset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inp, label = data[0],data[1]\n",
        "    inp = inp.to(device)\n",
        "    label = label.to(device)\n",
        "    out = None\n",
        "    out = model(inp)\n",
        "    loss = criterion(out, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%1000==0:\n",
        "      print(f'loss at epoch {e} and batch {i} is '+\"Loss: {:.3f}\".format(loss))\n",
        "      print(torch.argmax(out[0]),label[0])\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbA6luFy3J16",
        "outputId": "78ed7232-2a95-4e4e-e1c2-fcfd146ab556"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is not available. Using CPU for training.\n",
            "loss at epoch 0 and batch 0 is Loss: 8.503\n",
            "tensor(4786) tensor(4408)\n"
          ]
        }
      ]
    }
  ]
}