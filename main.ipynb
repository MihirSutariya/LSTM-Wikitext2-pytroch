{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 113161), (',', 99913), ('.', 73388), ('of', 56889), ('and', 50603), ('in', 39453), ('to', 39190), ('a', 34237), ('=', 29570), ('\"', 28309), ('was', 20985), ('The', 17602), ('@-@', 16906), ('that', 14135), ('as', 14021), (\"'s\", 14002), ('on', 13678), ('for', 13307), ('with', 12606), ('by', 12148), (')', 12004), ('(', 11992), ('is', 11637), ('from', 8941), ('his', 8395), ('at', 8186), ('were', 7330), ('it', 6748), ('he', 5947), ('an', 5895), ('had', 5698), ('which', 5543), ('In', 5525), ('be', 4787), ('are', 4669), (';', 4224), ('their', 4127), ('but', 4035), ('first', 3978), ('â€“', 3934), ('not', 3906), (':', 3806), ('also', 3746), ('its', 3697), ('or', 3636), ('her', 3486), ('have', 3442), ('one', 3351), ('has', 3322), ('been', 3257), ('two', 3238), ('@.@', 3194), ('this', 3079), ('who', 2963), ('they', 2941), (\"'\", 2870), ('He', 2759), ('@,@', 2699), ('after', 2640), ('It', 2522), ('time', 2451), ('into', 2440), ('other', 2433), ('would', 2320), ('more', 2319), ('1', 2253), ('all', 2100), ('when', 2036), ('I', 2026), ('over', 2015), ('2', 1982), ('she', 1981), ('during', 1973), ('only', 1948), ('A', 1919), ('game', 1846), ('up', 1843), ('him', 1840), ('about', 1833), ('three', 1803), ('most', 1778), ('between', 1768), ('than', 1759), ('out', 1707), ('while', 1684), ('later', 1682), ('season', 1641), ('made', 1635), ('3', 1601), ('such', 1532), ('year', 1507), ('used', 1505), ('new', 1501), ('where', 1490), ('them', 1481), ('This', 1481), ('some', 1469), ('On', 1462), ('000', 1447), ('song', 1444)]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "import random\n",
    "vocab = 50000\n",
    "text_len = 20\n",
    "# Load Wikitext-103 dataset\n",
    "dataset = load_dataset(path=\"wikitext\", name='wikitext-2-raw-v1', split=\"train\")\n",
    "# Define tokenizer\n",
    "tokenizer = lambda x: x.split()  # Simple tokenizer splitting by space\n",
    "# Tokenize the text and count frequency\n",
    "counter = Counter()\n",
    "train_data=[]\n",
    "for example in dataset:\n",
    "    tokens = tokenizer(example[\"text\"])\n",
    "    if len(tokens)>=text_len:\n",
    "      train_data.extend(tokens)\n",
    "    counter.update(tokens)\n",
    "\n",
    "# Select the 5000 most common words\n",
    "most_common_words = counter.most_common(vocab-1)\n",
    "word_to_index = {word: i for i, (word, _) in enumerate(most_common_words)}\n",
    "\n",
    "# values = list(word_to_index.values())\n",
    "# random.shuffle(values)\n",
    "# for i,k in enumerate(word_to_index.keys()):\n",
    "#   word_to_index[k] = values[i]\n",
    "\n",
    "\n",
    "# Define a function to convert text to one-hot encoding\n",
    "def text_to_one_hot(text):\n",
    "    token = text\n",
    "    index = word_to_index.get(token, vocab-1)\n",
    "    return index\n",
    "\n",
    "# Convert example text into one-hot encoding as a torch tensor\n",
    "train_w_data = [text_to_one_hot(word) for word in train_data]\n",
    "train_data = train_w_data\n",
    "print(most_common_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Text_dataset(Dataset):\n",
    "\n",
    "  def __init__(self, text_list, text_len):\n",
    "    self.data= text_list\n",
    "    self.text_len = text_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)//self.text_len\n",
    "\n",
    "  # This will\n",
    "  def __getitem__(self,i):\n",
    "    return torch.tensor(self.data[i*self.text_len:i*self.text_len+29]), torch.tensor(self.data[i*self.text_len+1:i*self.text_len+30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Gen(nn.Module):\n",
    "  def __init__(self,input_size=1000, hidden_size=1024, hidden_layer=1,embedding_size=512, batch_size=20):\n",
    "    super(LSTM_Gen, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.hidden_layer = hidden_layer\n",
    "    self.embedding_size = embedding_size\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "    self.embedding = torch.nn.Embedding(self.input_size, self.embedding_size)\n",
    "    self.lstm = torch.nn.LSTM(self.embedding_size, self.hidden_size, self.hidden_layer, batch_first=True)\n",
    "    self.fc1 = torch.nn.Linear(self.hidden_size,self.input_size)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "\n",
    "  def forward(self,x,hidden):\n",
    "    x = self.embedding(x)\n",
    "    x = self.dropout(x)\n",
    "    x, h = self.lstm(x, hidden)\n",
    "    out = self.fc1(self.dropout(x))\n",
    "    return out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "check_point = 19\n",
    "\n",
    "batch_size = 30\n",
    "hidden_size = 2048\n",
    "hidden_layer = 2\n",
    "embedding_size = 1024\n",
    "epoch = 20\n",
    "learning_rate = 0.001\n",
    "clip = 0.25\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set device to CUDA\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available! Using GPU for training.\")\n",
    "else:\n",
    "    # Set device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU for training.\")\n",
    "dset = Text_dataset(train_data,text_len)\n",
    "model = LSTM_Gen(vocab,hidden_size,hidden_layer,embedding_size,batch_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "if check_point>=0:\n",
    "  model.load_state_dict(torch.load(f'model_{check_point}.pth'))\n",
    "model.train()\n",
    "for e in range(check_point+1,epoch):\n",
    "  train_loader = DataLoader(dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  for i, data in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    inp, label = data[0],data[1]\n",
    "    inp = inp.to(device)\n",
    "    label = label.to(device)\n",
    "    out = None\n",
    "    hidden = (torch.zeros(hidden_layer, inp.shape[0], hidden_size).to(device),torch.zeros(hidden_layer,inp.shape[0], hidden_size).to(device))\n",
    "    out,_ = model(inp,hidden)\n",
    "    out = out.view(-1,vocab)\n",
    "    label = label.view(-1)\n",
    "    loss = criterion(out, label)\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'weight' in name:\n",
    "    #         print(name, param.grad)\n",
    "    optimizer.step()\n",
    "\n",
    "    if i%100==0:\n",
    "      print(f'loss at epoch {e} and batch {i} is '+\"Loss: {:.3f}\".format(loss))\n",
    "      print(torch.argmax(out[10]).item(),label[10].item())\n",
    "  torch.save(model.state_dict(), f'model_{e}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smith , the <unk> <unk> , and the <unk> <unk> , which was serialised in the \n",
      "United States on November 30 , 2003 . The episode was watched by 8 @.@ \n",
      "8 million viewers , and the second single in Europe . The album was the \n",
      "first single from the album , and it was released on the album . It \n",
      "was released on July 26 , 2012 , and the album peaked at number one \n",
      "on the Billboard 200 . It was certified double platinum by the Recording Industry Association \n",
      "of America ( RIAA ) , denoting shipments of ten "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# start = word_to_index['Germany']\n",
    "start = 1500\n",
    "input = torch.tensor([[start]]).to(device)\n",
    "print(most_common_words[start][0],end=' ')\n",
    "out = 0\n",
    "hidden = (torch.zeros(hidden_layer, 1, hidden_size).to(device),torch.zeros(hidden_layer,1,hidden_size).to(device))\n",
    "i=0\n",
    "while( i!=100):\n",
    "  out, hidden = model(input,hidden)\n",
    "  out = torch.argmax(out[0][0]).item()\n",
    "  input = torch.tensor([[out]]).to(device)\n",
    "  if out!=vocab-1:\n",
    "    print(most_common_words[out][0],end=' ')\n",
    "  else:\n",
    "    print('<unk>',end=' ')\n",
    "  if (i+1)%15==0:\n",
    "    print()\n",
    "  i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
