{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Senjō', 'no', 'Valkyria', '3', ':', '<unk>', 'Chronicles', '(', 'Japanese', ':', '戦場のヴァルキュリア3', ',', 'lit', '.', 'Valkyria', 'of', 'the', 'Battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'Valkyria', 'Chronicles', 'III', 'outside', 'Japan'], ['The', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'Valkyria', 'Chronicles', 'II', '.', 'While', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series'], ['It', 'met', 'with', 'positive', 'sales', 'in', 'Japan', ',', 'and', 'was', 'praised', 'by', 'both', 'Japanese', 'and', 'western', 'critics', '.', 'After', 'release', ',', 'it', 'received', 'downloadable', 'content', ',', 'along', 'with', 'an', 'expanded']]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "import random\n",
    "vocab = 30000\n",
    "text_len = 30\n",
    "# Load Wikitext-103 dataset\n",
    "dataset = load_dataset(path=\"wikitext\", name='wikitext-2-raw-v1', split=\"train\")\n",
    "# Define tokenizer\n",
    "tokenizer = lambda x: x.split()  # Simple tokenizer splitting by space\n",
    "\n",
    "# Tokenize the text and count frequency\n",
    "counter = Counter()\n",
    "train_data=[]\n",
    "for example in dataset:\n",
    "    tokens = tokenizer(example[\"text\"])\n",
    "    if len(tokens)>=text_len:\n",
    "      train_data.append(tokens[:text_len])\n",
    "    counter.update(tokens)\n",
    "\n",
    "# Select the 5000 most common words\n",
    "most_common_words = counter.most_common(vocab-1)\n",
    "word_to_index = {word: i for i, (word, _) in enumerate(most_common_words)}\n",
    "\n",
    "# values = list(word_to_index.values())\n",
    "# random.shuffle(values)\n",
    "# for i,k in enumerate(word_to_index.keys()):\n",
    "#   word_to_index[k] = values[i]\n",
    "\n",
    "\n",
    "# Define a function to convert text to one-hot encoding\n",
    "def text_to_one_hot(text):\n",
    "    token = text\n",
    "    index = word_to_index.get(token, vocab-1)\n",
    "    return index\n",
    "\n",
    "# Convert example text into one-hot encoding as a torch tensor\n",
    "print(train_data[:3])\n",
    "train_w_data = []\n",
    "for i in train_data:\n",
    "  train_w_data.append([text_to_one_hot(word) for word in i])\n",
    "train_data = train_w_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Text_dataset(Dataset):\n",
    "\n",
    "  def __init__(self, text_list):\n",
    "    self.data= text_list\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  # This will\n",
    "  def __getitem__(self,i):\n",
    "    return torch.tensor(self.data[i][:-1]), torch.tensor(self.data[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Gen(nn.Module):\n",
    "  def __init__(self,input_size=1000, hidden_size=1024, hidden_layer=1,embedding_size=512, batch_size=20):\n",
    "    super(LSTM_Gen, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.hidden_layer = hidden_layer\n",
    "    self.embedding_size = embedding_size\n",
    "    self.dropout = nn.Dropout(p=0.5)\n",
    "    self.embedding = torch.nn.Embedding(self.input_size, self.embedding_size)\n",
    "    self.lstm = torch.nn.LSTM(self.embedding_size, self.hidden_size, self.hidden_layer, batch_first=True)\n",
    "    self.fc1 = torch.nn.Linear(self.hidden_size,self.input_size)\n",
    "    self.batch_size = batch_size\n",
    "    self.embedding.weight = self.fc1.weight\n",
    "\n",
    "\n",
    "  def forward(self,x,hidden):\n",
    "    x = self.embedding(x)\n",
    "    x = self.dropout(x)\n",
    "    x, _ = self.lstm(x, hidden)\n",
    "    out = self.fc1(self.dropout(x))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU for training.\n",
      "loss at epoch 0 and batch 0 is Loss: 299.083\n",
      "[8784, 8784, 5643, 29111, 14544, 271, 23793, 20766, 271, 5643, 5666, 5643, 11786, 18742, 5666, 18742, 11786, 5643, 20606, 2565, 5643, 271, 5643, 7194, 26243, 9456, 18537, 5643, 17752] tensor([ 1068,    11,  2202,     1,     0,  3395, 24732,     1,    24,   638,\n",
      "            1, 11175, 29999,     1,     4,    40,   440,     1,  2451,  1068,\n",
      "           11,  2202,     4, 29999,  1068,    11,  2202,     1,  2844])\n",
      "loss at epoch 0 and batch 10 is Loss: 232.792\n",
      "[0, 1, 0, 1, 2, 3, 3, 1, 1, 3, 1, 1, 3, 4, 2, 1, 4, 1, 1, 6, 5, 1, 3, 0, 3, 1, 4, 1, 4] tensor([ 1750,     3,    24,   415,    13, 16591,   281,     6,  5609,     1,\n",
      "        17418,   382,   783,     6,     0, 26662,     4,    29,  3491,    25,\n",
      "            0,  1320,     6,   207,  5487,     2,   359,     7,    61])\n",
      "loss at epoch 0 and batch 20 is Loss: 222.252\n",
      "[29999, 0, 29999, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 29999, 29999, 0, 0, 1, 3, 0, 29999, 1, 1] tensor([    0,   518,    10,   589,    71,     1,   102,  2435,   392,    25,\n",
      "        29999,   247,    19, 29999, 29999,  1150,   504,     1,    35,    58,\n",
      "           27,  8419,    17,    58,    31,   431,    72,     0,   301])\n",
      "loss at epoch 0 and batch 30 is Loss: 218.286\n",
      "[0, 29999, 29999, 1, 29999, 1, 29999, 29999, 29999, 29999, 1, 29999, 29999, 1, 0, 1, 1, 3, 29999, 29999, 29999, 1, 1, 1, 29999, 29999, 0, 0, 1] tensor([ 217,    6,   72,  479,  256,    4,  517,  127,   17,    0,  369,  372,\n",
      "           7, 2720,  171,   11,  193,    2,   32,  258,  256,    1,   29, 6508,\n",
      "         171,   49,  115,  507,    1])\n",
      "loss at epoch 0 and batch 40 is Loss: 216.651\n",
      "[0, 0, 1, 29999, 0, 29999, 0, 0, 0, 0, 1, 29999, 1, 29999, 0, 29999, 0, 1, 0, 0, 0, 0, 0, 29999, 0, 0, 29999, 1, 0] tensor([29999,    17, 29999, 29999,    42, 29999,  1787,  1637,     6,    48,\n",
      "            3,     0, 23767,   779,     3,  4861,     1,     4,    68,    29,\n",
      "         3090, 24952,    29,    22,  3462,     5,    47,  2640,     5])\n",
      "loss at epoch 0 and batch 50 is Loss: 214.381\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 3, 1, 1, 29999, 1, 0, 1, 0, 1, 1, 1, 1, 0] tensor([  416,  6025,  2579, 10832,  5892,    27,    50,  2579, 10832,  5892,\n",
      "          278,    19,   416,  6025,    21, 19662,    20,    16,   765,  1002,\n",
      "           21, 23279,    20,     6,  2515,     2,    12,    50,   729])\n",
      "loss at epoch 0 and batch 60 is Loss: 221.714\n",
      "[0, 0, 1, 0, 29999, 29999, 29999, 0, 0, 0, 1, 0, 29999, 3, 1, 29999, 0, 0, 0, 1, 3, 1, 29999, 29999, 3, 1, 0, 1, 29999] tensor([  238,     3, 13027,  1025,    30,  2488,     5,     0,   827,   871,\n",
      "           16,  2817,     6,     0,   294,   170, 17002,    13,  2496,     1,\n",
      "            4,   493,  1029,   596,  2154,     2,    12,   118,   714])\n",
      "loss at epoch 0 and batch 70 is Loss: 213.460\n",
      "[0, 0, 29999, 29999, 29999, 29999, 29999, 0, 29999, 29999, 29999, 1, 1, 29999, 29999, 1, 1, 1, 1, 0, 29999, 0, 0, 1, 0, 0, 29999, 29999, 29999] tensor([2699,   14,    0, 2389,  557,   26,    0,  674,    3,    0,  230,  148,\n",
      "           3,    0, 3482,    1,  199,  201,    1, 4552,    2,   12, 2114,   10,\n",
      "        3683,    5, 1655,   26,  145])\n",
      "loss at epoch 0 and batch 80 is Loss: 209.981\n",
      "[0, 29999, 1, 1, 29999, 29999, 3, 1, 0, 29999, 1, 1, 2, 29999, 1, 1, 0, 0, 29999, 29999, 29999, 1, 29999, 1, 29999, 1, 1, 0, 29999] tensor([ 3213,     3,     0,   666,   350,  1845,   192,   247,     5,     7,\n",
      "          889,  5792,     3,  1386,  1192,     1,     4,    25,   200, 19646,\n",
      "          143,   199, 19826,   126, 29999,  4187,    27,   279,     2])\n",
      "loss at epoch 0 and batch 90 is Loss: 217.673\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 29999, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0] tensor([ 7870, 29999,     3,   458,   253,     0,   719,  1604,  3018,  1556,\n",
      "            6,     0,  1706,     3, 14611,    64,    11,  5805,    11,  1881,\n",
      "        25422,     3,  2596,  1604,   755,     0,   625,     2,   485])\n",
      "loss at epoch 0 and batch 100 is Loss: 208.038\n",
      "[0, 29999, 0, 29999, 29999, 0, 1, 1, 29999, 29999, 1, 29999, 1, 29999, 29999, 29999, 29999, 1, 29999, 0, 1, 1, 29999, 0, 1, 0, 29999, 29999, 29999] tensor([19063,   226,    60,    34,     6,  2875,    16,    94,   127,     1,\n",
      "         1312, 29999,   892,    15,    29,    60,   163,     5,   610,     0,\n",
      "          713,   775,     5,  1073,    18,     0,  3285, 24226,     3])\n",
      "loss at epoch 0 and batch 110 is Loss: 210.294\n",
      "[0, 0, 0, 0, 29999, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0] tensor([  298,    11, 12317,    21,   115,     6,    20, 18426, 14761,  1463,\n",
      "            0,  3925,    62, 12549,     1,   283,    58,    75,   453,    77,\n",
      "            5,     0,   269,  2171,     2, 29999,   138,    37,    47])\n",
      "loss at epoch 0 and batch 120 is Loss: 209.306\n",
      "[0, 0, 29999, 0, 0, 0, 0, 29999, 0, 0, 29999, 1, 29999, 0, 29999, 29999, 0, 0, 29999, 29999, 29999, 0, 0, 0, 0, 0, 0, 29999, 1] tensor([3180,   40,  315, 1032, 1063,    7,   98,   83,    5,  345, 3495,  304,\n",
      "           3,    0,   87, 2914,  441,    1,   35,   27, 8143,   59,   50, 1598,\n",
      "         784, 1553,   15,   98,   13])\n",
      "loss at epoch 0 and batch 130 is Loss: 204.937\n",
      "[29999, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 29999, 29999, 29999, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 29999, 1] tensor([    0,    39,  7803,    14,   200,    76,     1,   551,     1, 29999,\n",
      "            4,  2401,  5401,     0,    85,  2937,    38, 29999,    10,   878,\n",
      "           19,   425,    80,    50,  3445,  2937,     2,   146,   122])\n",
      "loss at epoch 0 and batch 140 is Loss: 204.695\n",
      "[0, 0, 29999, 1, 1, 29999, 29999, 29999, 1, 29999, 0, 29999, 1, 29999, 29999, 29999, 29999, 29999, 1, 1, 29999, 1, 0, 29999, 1, 1, 1, 1, 1] tensor([  130,    15,     0,  1249,  1035,    26,   611,   218,   685,     3,\n",
      "         8935,   910,     2, 29999,   383,     0,  3361,  1130,     9,  1403,\n",
      "          163,     0, 29999,  1454,     1, 12439,     4, 29999,     1])\n",
      "loss at epoch 0 and batch 150 is Loss: 205.988\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 29999, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] tensor([ 4801,   168,   228,   685,    96,     0,    87,   664,     3,  6925,\n",
      "            1, 29999, 29999,     1,    10, 11968,     6,     1,    26,    79,\n",
      "          214,    42,   240,   336,    61,     1,    26,     0, 12821])\n",
      "loss at epoch 0 and batch 160 is Loss: 207.821\n",
      "[0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 29999, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0] tensor([  196,    10,   245,     5,     0, 21270,  1967,  5302,    21,  1871,\n",
      "           41,   693,    20,     1,     7,   101,   209,    41,  6207,   441,\n",
      "         2388,   128,    43,  8301,    18,     0,  2925,  1967, 22830])\n",
      "loss at epoch 0 and batch 170 is Loss: 204.725\n",
      "[0, 0, 0, 1, 29999, 0, 1, 29999, 1, 29999, 29999, 0, 29999, 0, 0, 0, 0, 1, 29999, 29999, 0, 1, 0, 1, 1, 29999, 1, 0, 29999] tensor([16019,     0,   346,     3, 29999,    13, 29999,  2771,     1,     0,\n",
      "         1727, 29999,  1942, 29999,   383,    15,  3203,    10,     9,  5543,\n",
      "           18, 17677,     4, 11959, 11249,    19,    72,    44,   320])\n",
      "loss at epoch 0 and batch 180 is Loss: 208.882\n",
      "[0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1] tensor([  261,  3409,    14,    64,   156,     5, 24116,     1,    35,   138,\n",
      "           37,   466,     5,     0,  9431,   188,     2, 29999,    19, 29999,\n",
      "            1,    69,  2795,     5,     0,   250,  1065,   839,     4])\n",
      "loss at epoch 0 and batch 190 is Loss: 204.132\n",
      "[0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1] tensor([   4, 2737, 3203,   47,  705,  335,  412,   18, 9029,  588,  321,   41,\n",
      "         274,    2, 9029, 3131,  214,   41,  145,   14, 1315, 3301,    1,    4,\n",
      "          76,   41,   64,   14, 5950])\n",
      "loss at epoch 0 and batch 200 is Loss: 206.831\n",
      "[0, 0, 0, 0, 0, 0, 0, 29999, 0, 29999, 0, 0, 0, 0, 0, 0, 29999, 0, 0, 0, 29999, 0, 0, 0, 0, 0, 0, 0, 0] tensor([  805,    21,   207,   653,     1, 19370,    41,   207,   201,     1,\n",
      "         9573,    20,    10,     7,  4100,   141,   134,   408,  1206,     1,\n",
      "         4684,     1,  3195,     1,     4,  5651,     6,     0,   369])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 20\n",
    "hidden_size = 1024\n",
    "hidden_layer = 2\n",
    "embedding_size = 1024\n",
    "epoch = 5\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00005\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Set device to CUDA\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available! Using GPU for training.\")\n",
    "else:\n",
    "    # Set device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU for training.\")\n",
    "dset = Text_dataset(train_data)\n",
    "model = LSTM_Gen(vocab,hidden_size,hidden_layer,embedding_size,batch_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for e in range(epoch):\n",
    "  train_loader = DataLoader(dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  for i, data in enumerate(train_loader):\n",
    "    inp, label = data[0],data[1]\n",
    "    inp = inp.to(device)\n",
    "    label = label.to(device)\n",
    "    out = None\n",
    "    hidden = (torch.zeros(hidden_layer, batch_size, hidden_size).to(device),torch.zeros(hidden_layer,batch_size, hidden_size).to(device))\n",
    "\n",
    "    out = model(inp,hidden)\n",
    "    out = out.permute(1,0,2)\n",
    "    label = label.permute(1,0)\n",
    "    loss = criterion(out[0], label[0])\n",
    "    for j in range(1,text_len-1):\n",
    "      loss += criterion(out[j], label[j])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i%10==0:\n",
    "      print(f'loss at epoch {e} and batch {i} is '+\"Loss: {:.3f}\".format(loss))\n",
    "      print([torch.argmax(ans).item() for ans in out[:,0,:]],label[:,0])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
